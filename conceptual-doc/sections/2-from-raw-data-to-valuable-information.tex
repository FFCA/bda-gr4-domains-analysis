\section{From Raw Data To Valuable Information}\label{sec:from-raw-data-to-valuable-information}

\subsection{Business Understanding ...}\label{subsec:businessunderstanding}
The given data includes DNS record information concerning '.de' domains, the country code top-level domains (ccTLD's)~\autocite[cf.][]{DENICeG.2021} allocated to the Federal Republic of Germany.
The comma-separated value file (CSV) with a size of about 500 megabytes contains \textit{the domain name, mail servers used, the domains target IPv4 address, and the timestamp, the information were fetched}.
As CSV files offer straightforward integration into software systems and programs supporting the format~\autocite[cf.][]{Hoffman.2018}, complex file conversion steps (possibly using proprietary software) are not necessary.
Nevertheless, checking the content for correct set delimiters, unnecessary characters, and reasonable data types for subsequent conversion is crucial.
Therefore, we implemented an ETL process to cover these checks and considerations, while ensuring a certain level of flexibility concerning the data sources.
An important part is the enhancement of the given data by statistics to gain more value out of the data.
Therefore, further data were collected as described in ~\ref{subsec:businessunderstanding}.
As the amount of data and the processes offer various analyses, the following objectives of the project were determined:
\begin{itemize}
    \item Follow the approach ‘everything as code’
    \item Validate existing and collect additional information
    \item Establish an ETL process and data preparation as a requirement for the analyses
    \item Analyses and deployment of the results
\end{itemize}
These processes and objectives as well as the explanations in the following sections are supposed to provide a detailed analysis of DNS record information within the context of big data.


% Beschreibungen und Komentare aus https://www.bigdata-insider.de/was-ist-crisp-dm-a-815478/

\subsection{Data Understanding ...}\label{subsec:dataunderstanding}
\begin{itemize}
    \item Erster Überblick über die Daten (Welche Daten, gibt es Probleme)
    \item Probelme insbeosndere in Bezug auf die im vorherigen Kapitel benannten Ziele/Aufgabenstellung/Vorgehensweise
\end{itemize}

\subsection{Data Preparation ...}\label{subsec:datapreparation}
Data preparation is an essential process beforehand any analysis and thus, should be well documented within the project.
Therefore, Jupyter notebooks containing the associated steps in code blocks were used to provide the processed information.
The code blocks were commented on or being accompanied by explanations to ensure transparency and to match best practices in coding~\autocite[cf.][]{Kosourova.2021}.
The file names are numbered so that the notebooks should be executed consecutively to ensure a clean and smooth delivery of the results.

At first, the required functions are loaded to make them available in the following notebooks.
Afterward, the ETL process is carried out to load the data from the PostgreSQL database, cleaning them (removing special characters and empty lines) and provide the basic data frame for further information processing.
Another enhancement is envisaged by certain checks of the data such as checking the occurrence of 'localhost' entries for mail servers and counts to determine the top ten records (e.g.\ a records) before data are stored in the database.
Furthermore, the existing information is checked for correctness as DNS entries and domains themselves could have changed in the meantime.
This is done by using Pythons dnspython (and various further) package(s) to send requests, fetch the information, and compare them with the original entries if necessary.
To ensure detailed analyses, it is important to collect suitable information which is why we decided to perform the following steps of data retrieval:
\begin{itemize}
    \item Current IP addresses and MX Servers (in comparison to the given data)
    \item HTTP status code per domain and redirects applied to certain domains
    \item Collection of details such as the number of nameservers used per domain and the availability of IPv6 per domain
    \item Company details concerning MX servers and the server of authority (SOA)
    \item Configuration details of the authoritative nameserver (refresh and minimum setting)
\end{itemize}
These steps were performed on extracts of the given dataset as its amount (around 4.8 million) could interfere with performant coding.
As this amount requires corresponding processing steps, parallelization is essential to achieve reasonable runtimes within each code block, which is why we use PySpark, offering native parallelization by using its variant of data frames~\autocite[cf.][]{Weber.2019}.

A key result of this step was the knowledge, that provided data set contains partially outdated information (e.g.\ A- and MX-records of a top-level domain), that some domains are re-directed or that requesting them raised an HTTP errors code.
The methods listed above were focused to encounter these issues.

\subsection{Modeling ...}\label{subsec:modeling}
% Im Rahmen der Modellierung werden die für die Aufgabenstellung geeigneten Methoden des Data Minings auf den in der Datenvorbereitung erstellten Datensatz angewandt. Typisch für diese Phase sind die Optimierung der Parameter und die Erstellung mehrerer Modelle.

\subsection{Evaluation ...}\label{subsec:evaluation}
% %Die Evaluierung sorgt für einen exakten Abgleich der erstellten Datenmodelle mit der Aufgabenstellung und wählt das am besten passende Modell aus.
\begin{itemize}
    \item Auswahl am besten passender Modelle (Vorschlag Felix: hier Darstellungen und Auswertungen)
    \item erste Erkenntnisse anreißen (Status Codes 200 > != 200)?
\end{itemize}

\subsection{Deployment ...}\label{subsec:deployment}
% Ergebnisaufbereitung
\begin{itemize}
    \item Dashboard (Datenfluss, Aufteilung, Inhalte)
    \item Aktualisierungsvorgang
\end{itemize}
