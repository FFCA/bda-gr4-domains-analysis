\section{From Raw Data To Valuable Information}\label{sec:from-raw-data-to-valuable-information}

\subsection{Business Understanding}\label{subsec:businessunderstanding}
The given data includes DNS record information concerning '.de' domains, the country code top-level domains (ccTLD's)~\autocite[cf.][]{DENICeG.2021} allocated to the Federal Republic of Germany.
The comma-separated value file (CSV) with a size of about 500 megabytes contains \textit{.de-related domain names, mail servers used, the domains target IPv4 address, and the timestamp, the information were fetched}.
As CSV files offer straightforward integration into software systems and programs supporting the format~\autocite[cf.][]{Hoffman.2018}, complex file conversion steps (possibly using proprietary software) are not necessary.
Nevertheless, checking the content for correct set delimiters, unnecessary characters, and determination of reasonable data types for subsequent conversion is crucial.
Therefore, we implemented an ETL process to cover these checks and considerations, while ensuring a certain level of flexibility concerning the data sources.

Another important part is the enhancement of the given data by statistics to gain more value out of the data.
As the amount of data and processes to handle them offer various analyses, the following objectives of the project were determined:
\begin{itemize}
    \item Follow the approach ‘everything as code’
    \item Validate existing and collect additional information
    \item Establish an ETL process and data preparation as a requirement for the analyses
    \item Analyses and deployment of the results
\end{itemize}
These objectives as well as the processes explained in the following sections are supposed to provide detailed analysis of DNS record information within the context of big data.



\subsection{Data Understanding}\label{subsec:dataunderstanding}
After the concrete goals and requirements have been defined in the Business Understanding phase, a close look is taken at the existing basic data.
The existing data is to be examined for processability, content and quality~\autocite[cf.][]{Semmelmann.2020}.
Initial analyses show that the data is available in a comma-separated value structure and is assigned to the context of the Domain Name System (DNS).
Furthermore, the dataset consists of 4 columns and 4860885 rows.
Due to the large amount of data, the use of parallel data processing technologies is inevitable.
To gain an understanding of the data, it is described below:

\begin{center}
    \begin{tabular}{||c c c c||} 
    \hline
    Column & Description & Occurrence & Nullable \\ [0.5ex] 
    \hline\hline
    A & Top-level domains, excluding subdomains & unique & false \\ 
    \hline
    B & MX record from name server & multiple & true \\
    \hline
    C & A-record of the respective domain & multiple & true \\
    \hline
    D & Dolumn D: Timestamp of scraping process & multiple & false
   \end{tabular}
   \end{center}



\subsection{Data Preparation}\label{subsec:datapreparation}
Data preparation is an essential process beforehand any analysis and thus, should be well documented within the project.
Therefore, Jupyter notebooks containing the associated steps in code blocks were used to provide the processed information.
The code blocks were commented on or being accompanied by explanations to ensure transparency and to match best practices in coding~\autocite[cf.][]{Kosourova.2021}.
The file names are numbered so that the notebooks should be executed consecutively to ensure a clean and smooth delivery of the results.

At first, the required functions are loaded to make them available in the following notebooks.
Afterward, the ETL process is carried out to load the data from the PostgreSQL database, cleaning them (removing special characters and empty lines) and provide the basic data frame for further information processing.
Another enhancement is envisaged by certain checks of the data such as checking the occurrence of 'localhost' entries for mail servers and counts to determine the top ten records (e.g.\ a records) before data are stored in the database.
Furthermore, the existing information is checked for correctness as DNS entries and domains themselves could have changed in the meantime.
This is done by using Pythons dnspython (and various further) package(s) to send requests, fetch the information, and compare them with the original entries if necessary.
To ensure detailed analyses, it is important to collect suitable information which is why we decided to perform the following steps of data retrieval:
\begin{itemize}
    \item Current IP addresses and MX Servers (in comparison to the given data)
    \item HTTP status code per domain and redirects applied to certain domains
    \item Collection of details such as the number of nameservers used per domain and the availability of IPv6 per domain
    \item Company details concerning MX- and nameservers
    \item Configuration details of the authoritative nameserver (refresh and minimum setting)
\end{itemize}
These steps were performed on extracts of the given dataset as its amount could interfere with performant coding.
As this amount requires corresponding processing steps, parallelization is essential to achieve reasonable runtimes within each code block, which is why we use PySpark, offering native parallelization by using its variant of data frames~\autocite[cf.][]{Weber.2019}.

A key result of this step was the knowledge, that provided data set contains partially outdated information (e.g.\ A- and MX-records of a top-level domain), that some domains are re-directed or that requesting them raised an HTTP errors code.
The methods listed above were focused to encounter these issues.

\subsection{Modeling}\label{subsec:modeling}
The following phase is one of the main points of the CRIPS-DM process, where machine learning methods are applied to the data to build predictive models~\autocite[cf.][]{Semmelmann.2020}.
Models used in data mining were not implemented within the project in favor of detailed descriptive analyses of the results gained by the steps described before.
The application of reasonable models like pattern recognition (e.g. Clustering methods) may require further data that could be realized in a follow-up project.
However, this is a challenging task concerning information retrieval, data storage, and loading which requires further enhancements and awareness of performance issues.



\subsection{Evaluation}\label{subsec:evaluation}
After preparing the data to build a predictive model, the final goal is to verify the predictive accuracy.
In this phase, the entire process including data availability, data quality, and use case coverage is analyzed~\autocite[cf.][]{Semmelmann.2020}.
As models were not used within this project, an evaluation of such was not done.
Re-evaluation of created models would be a continuous step in the project to keep track of changing requirements (analyses) and of the frequent changes of the data justified on the given context.


\subsection{Analysis tasks}\label{subsec:analysistasks}
The analyses within the dashboard are realized using key performance indices, to provide a quick overview concerning important facts.

We decided to express them either as a percentage, an amount, or an average of the respective index, to give a uniform and comprehensible presentation of such.
If appropriate, we used pie charts (always sorted descending) to depict the number of certain values (e. g. the number of nameservers per city, the respective providers are located).
Bar charts constitute a key graph used to convey the results found within the project and are always sorted in descending.

We decided to include these KPIs and graphs to provide a minimalistic, yet reasonable and informative dashboard.
The detailed description of the dashboard tabs can be seen in section~\ref{subsec:dashboard-sections-(kpis-and-charts)}



\subsection{Deployment}\label{subsec:deployment}
As deployment is an essential part of the provision of the results of the project, it was aimed to follow the approach 'everything as code'.

The successful realization of this objective requires several techniques and software to work properly, which is why detailed descriptions of the implementation within this project are available in section~\ref{sec:project-realization}.