\section{From Raw Data To Valuable Information}\label{sec:from-raw-data-to-valuable-information}

\subsection{Business Understanding ...}\label{subsec:businessunderstanding}
\begin{itemize}
    \item Ziele (Everything as code, Analysen, Dockerisierung, Notebooks)
    \item Besonderheiten der Daten (Fehlende, veraltete, unvollständige Angaben) und (Gegen-)Maßnahmen
    \item Definition Ausfgabenstellung
    \item Grobe Vorgehensweise
\end{itemize}

The given data represent DNS specifications concerning '.de' domains, country code top-level domains (ccTLD's)~\autocite[cf.][]{DENICeG.03.07.2021} of the Federal Republic of Germany.
The objective, gaining insight into the given information, requires the definition of processes covering technological, analytical and organizational aspects to ensure the success of the project.

% Beschreibungen und Komentare aus https://www.bigdata-insider.de/was-ist-crisp-dm-a-815478/

\subsection{Data Understanding ...}\label{subsec:dataunderstanding}
\begin{itemize}
    \item Erster Überblick über die Daten (Welche Daten, gibt es Probleme)
    \item Probelme insbeosndere in Bezug auf die im vorherigen Kapitel benannten Ziele/Aufgabenstellung/Vorgehensweise
\end{itemize}

\subsection{Data Preparation ...}\label{subsec:datapreparation}
Data preparation is an essential process beforehand any analysis and thus, should be well documented within the project.
Therefore, Jupyter notebooks containing the associated steps in code blocks were used to provide the processed information.
The code blocks were commented on or being accompanied by explanations to ensure transparency and to match best practices in coding~\autocite[cf.][]{Kosourova.23.4.2021}.
The file names are numbered so that the notebooks should be executed consecutively to ensure a clean and smooth delivery of the results.

At first, the required functions are loaded to make them available in the following notebooks.
Afterward, the ETL process is carried out to load the data from the PostgreSQL database, cleaning them (removing special characters and empty lines) and provide the basic data frame for further information processing.
Another enhancement is envisaged by certain checks of the data such as checking the occurrence of 'localhost' entries for mail servers and counts to determine the top ten records (e.g.\ a records) before data are stored in the database.
Furthermore, the existing information is checked for correctness as DNS entries and domains themselves could have changed in the meantime.
This is done by using Pythons dnspython (and various further) package(s) to send requests, fetch the information, and compare them with the original entries if necessary.
To ensure detailed analyses, it is important to collect suitable information which is why we decided to perform the following steps of data retrieval:
\begin{itemize}
    \item Current IP addresses and MX Servers (in comparison to the given data)
    \item HTTP status code per domain and redirects applied to certain domains
    \item Collection of details such as the number of nameservers used per domain and the availability of IPv6 per domain
    \item Company details concerning MX servers and the server of authority (SOA)
    \item Configuration details of the authoritative nameserver (refresh and minimum setting)
\end{itemize}
These steps were performed on extracts of the given dataset as its amount (around 4.8 million) could interfere with performant coding.
As this amount requires corresponding processing steps, parallelization is essential to achieve reasonable runtimes within each code block, which is why we use PySpark, offering native parallelization by using its variant of data frames~\autocite[cf.][]{Weber.21.1.2019} .

A key result of this step was the knowledge, that provided data set contains partially outdated information (e.g.\ A- and MX-records of a top-level domain), that some domains are re-directed or that requesting them raised an HTTP errors code.
The methods listed above were focused to encounter these issues.

\subsection{Modeling ...}\label{subsec:modeling}
% Im Rahmen der Modellierung werden die für die Aufgabenstellung geeigneten Methoden des Data Minings auf den in der Datenvorbereitung erstellten Datensatz angewandt. Typisch für diese Phase sind die Optimierung der Parameter und die Erstellung mehrerer Modelle.

\subsection{Evaluation ...}\label{subsec:evaluation}
% %Die Evaluierung sorgt für einen exakten Abgleich der erstellten Datenmodelle mit der Aufgabenstellung und wählt das am besten passende Modell aus.
\begin{itemize}
    \item Auswahl am besten passender Modelle (Vorschlag Felix: hier Darstellungen und Auswertungen)
    \item erste Erkenntnisse anreißen (Status Codes 200 > != 200)?
\end{itemize}

\subsection{Deployment ...}\label{subsec:deployment}
% Ergebnisaufbereitung
\begin{itemize}
    \item Dashboard (Datenfluss, Aufteilung, Inhalte)
    \item Aktualisierungsvorgang
\end{itemize}
