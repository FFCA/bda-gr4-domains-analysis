{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624f38a6-d4b2-4dc0-a56d-d1c2c642535d",
   "metadata": {},
   "source": [
    "# Enhancement of collected data\n",
    "\n",
    "This notebook performs a domain analysis based on the data `real_domains.csv`. First, it checks the correctness of the specified `A records` and `MX records`. The current A-records and MX-records of the top-level domain are determined. In the next step, the `redirects` and `HTTP status codes` are obtained from the top-level domains. The Python package called `dnspython` is used for the determination. In addition, a further analysis of the MX records determines their locations and the corresponding ASNs. Free IP geolocation databases from MaxMind are used for this.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, the required installations and imports are made, the session is initialized and the basis data is collected from the Postgres database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6289df2-604f-4041-9b85-bf72f438b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations\n",
    "!pip install ipynb\n",
    "!pip install dnspython\n",
    "!pip install geoip2\n",
    "\n",
    "# Required imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# PostgreSQL access data\n",
    "host = \"bda_gr4_database\"\n",
    "port = \"5432\"\n",
    "database = \"domainanalysis\"\n",
    "user = \"postgres\"\n",
    "password = \"postgres\"\n",
    "\n",
    "# PostgreSQL connection url\n",
    "connection = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"domain_analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from the database\n",
    "domains_df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", connection) \\\n",
    "                .option(\"dbtable\", \"domain\") \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "\n",
    "# Display the data frame\n",
    "domains_df.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e96ed-e7d7-44e0-9a4c-2b3bae564f68",
   "metadata": {},
   "source": [
    "## 1. Validation of A and MX records\n",
    "\n",
    "In the following chapter, the functions are imported from the notebook `Functions.ipynb` in order to use them for the definition of the `UDFs`. In doing so, the current `a-records` and `mx-records` are to be determined from the `top-level-domain`.  If the records cannot be determined, the returned errors are stored in separate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8977420-ff1b-4f23-8fc4-39211203e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Funtions.ipynb\n",
    "from ipynb.fs.full.Functions import getARecords, getARecords_error, getMXRecords, getMXRecords_error\n",
    "\n",
    "# Creating of UDF's\n",
    "udf_getARecords = udf(getARecords, ArrayType(StringType()))\n",
    "udf_getARecords_error = udf(getARecords_error, IntegerType())\n",
    "udf_getMXRecords = udf(getMXRecords, ArrayType(StringType()))\n",
    "udf_getMXRecords_error = udf(getMXRecords_error, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fb87f-9904-45f4-8887-bdeb1d69d662",
   "metadata": {},
   "source": [
    "Next, the A and MX records are checked by calling the UDFs and creating the returned results in new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc074022-afb0-4293-9e14-03a4ea7249da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new columns with the results\n",
    "domains_checked_df = domains_df.withColumn(\"a_record_checked\", udf_getARecords(\"top_level_domain\")) \\\n",
    "                            .withColumn(\"a_record_checked_error\", udf_getARecords_error(\"top_level_domain\")) \\\n",
    "                            .withColumn(\"mx_record_checked\", udf_getMXRecords(\"top_level_domain\")) \\\n",
    "                            .withColumn(\"mx_record_checked_error\", udf_getMXRecords_error(\"top_level_domain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86919685-ab99-4b42-8065-e7f3211b0b35",
   "metadata": {},
   "source": [
    "In preparation for writing to the database, the data frame is put into the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcae96a-9f79-4588-86be-3693320b8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the order of the data frame\n",
    "domains_checked_df = domains_checked_df.select(\"top_level_domain\", \"a_record\", \"a_record_checked\", \"a_record_checked_error\", \"mx_record\", \"mx_record_checked\", \"mx_record_checked_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d19651-857d-4e01-b100-965418716252",
   "metadata": {},
   "source": [
    "Last but not least, let's check if the records have either not been changed, changed completely or partially in comparison to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83143f17-45b2-47bd-a0c5-82937641fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occrence of checked A-records\n",
    "a_record_checked_count = domains_df.withColumn('a_record_checked', explode(col('a_record'))) \\\n",
    "        .groupBy('a_record_checked') \\\n",
    "        .count()\n",
    "\n",
    "# Count the occrence of checked MX-records\n",
    "mx_record_checked_count = domains_df.withColumn('mx_record_checked', explode(col('mx_record'))) \\\n",
    "        .groupBy('mx_record_checked') \\\n",
    "        .count()\n",
    "\n",
    "# Finally, create new data frames only containing the top 10 A-/MX-records\n",
    "a_record_count_top_ten_df = a_record_checked_count.orderBy(['count'], ascending = [False]).limit(10)\n",
    "mx_record_count_top_ten_df = mx_record_checked_count.orderBy(['count'], ascending = [False]).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd369bd-2215-4521-b4f3-55534c250fa9",
   "metadata": {},
   "source": [
    "Finally, the columns `a_record` and `mx_record` are removed from the `domains_checked_df` data frame. Furthermore, the first 15 rows of the data frame are displayed as a check for writing to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20bf19-d35c-407f-8d71-bbb7818d1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns and show the data frame\n",
    "domains_checked_df = domains_checked_df.drop(\"a_record\").drop(\"mx_record\")\n",
    "domains_checked_df.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4f9e4-a6d3-4ac7-82b0-a4e22ea6c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data frame\n",
    "a_record_count_top_ten_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56227837-4fc9-4b0a-b2f4-978c29782d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data frame\n",
    "mx_record_count_top_ten_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536d2be-19d2-40e6-ac3e-ff6c4c138296",
   "metadata": {},
   "source": [
    "After the generated data frames have been checked, they must be written to the PostgreSQL database for data visualisation. To enable a faster write speed, `repartition` and `batchsize` are specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00c012-7ae2-43de-8194-5d38d474d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data frames to the PostgreSQL database\n",
    "a_record_count_top_ten_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"a_record_checked_count_global\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "mx_record_count_top_ten_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"mx_record_checked_count_global\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "domains_checked_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"domain_records_checked\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f96af-9be4-45df-a0f4-5fe10cdae227",
   "metadata": {},
   "source": [
    "## 2. Determination of redirects and HTTP status codes\n",
    "\n",
    "This chapter contains the definition of the `redirects` and `HTTP status` codes to the top level domains. In the following, the functions are imported from the function notebook `Functions.ipynb` in order to use them for the definition of the `UDFs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155f430-3b6b-4991-9a70-32e3930f1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Funtions.ipynb\n",
    "from ipynb.fs.full.Functions import getRedirectUrl, getStatusCodeUrl\n",
    "\n",
    "# Creating of UDF's\n",
    "udf_getRedirectUrl = udf(getRedirectUrl, StringType())\n",
    "udf_getStatusCodeUrl = udf(getStatusCodeUrl, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794f7fb-8359-408b-86df-c6acc81c2df4",
   "metadata": {},
   "source": [
    "In this block, the `redirections` and the `HTTP status codes` are determined by calling the UDF functions. For this purpose, the column `top_level_domain` is passed as a parameter. Subsequently, the redirections and status codes are stored in new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e15e74-32b1-4a1f-9d68-d5a68e92cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new columns with the results\n",
    "domains_redirect_df = domains_df.withColumn(\"redirection\", udf_getRedirectUrl(\"top_level_domain\")) \\\n",
    "                                .withColumn(\"status_code\", udf_getStatusCodeUrl(\"top_level_domain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443747b0-e267-412b-9e57-def5d2ee12aa",
   "metadata": {},
   "source": [
    "For storage in the PostgreSQL database, the `a_record` and `mx_record` columns are removed from the `domains_redirect_df` data frame and the first 15 rows of the data frame are displayed for checking for writing to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750fe84-993f-4706-9671-850ead9941d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns and show the data frame\n",
    "domains_redirect_df = domains_redirect_df.drop(\"a_record\").drop(\"mx_record\")\n",
    "domains_redirect_df.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc47be-3563-4de2-a152-191d067fb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_redirect_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"domain_redirection\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9d889-f1ed-40ae-b6fe-c7ce7ad70ee2",
   "metadata": {},
   "source": [
    "## 3. MX record location and ASN determination\n",
    "\n",
    "In this section, the `locations` and `providers` are identified based on the ANS using the MX records already checked. For each MX record the iso code, location, postal code, latitude, longitude and the organisation of the autonomous system are presented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fc9ec-7879-460b-8b75-2983200295cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the database\n",
    "domain_records_checked_df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", connection) \\\n",
    "                .option(\"dbtable\", \"domain_records_checked\") \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "\n",
    "# Drop columns\n",
    "domain_records_checked_df = domain_records_checked_df.drop(\"a_record_checked\").drop(\"a_record_checked_error\").drop(\"mx_record_checked_error\")\n",
    "\n",
    "# Display the data frame\n",
    "domain_records_checked_df.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62db54e-98cd-4a88-bd32-544ec1eefe7b",
   "metadata": {},
   "source": [
    "Next, the functions from the function notebook `Functions.ipynb` that are to be used for the definition of the `UDFs` are imported. After that, StructTypes of StructFields are created to define the return types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d8eec-adfb-4994-bd90-9b96dad33aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Funtions.ipynb\n",
    "from ipynb.fs.full.Functions import getARecords, getGeoLite2_Location, getGeoLite2_ASN\n",
    "\n",
    "schema_location = StructType([\n",
    "    StructField(\"iso_code\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"postal\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True)])\n",
    "\n",
    "schema_asn = StructType([\n",
    "    StructField(\"autonomous_system_organization\", StringType(), True)])\n",
    "\n",
    "udf_getARecords = udf(getARecords, ArrayType(StringType()))\n",
    "udf_getGeoLite2_Location = udf(getGeoLite2_Location, schema_location)\n",
    "udf_getGeoLite2_ASN = udf(getGeoLite2_ASN, schema_asn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05088e93-67c7-4808-af29-ae165f95907f",
   "metadata": {},
   "source": [
    "In order to be able to perform the location and provider query, the ip address of the mx records must first be found out. Based on this, the `locations` and the `providers` are determined and the data frame `domains_mx_record_geolite2_df` is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcfbbd-673a-4d99-9edf-dcc3e80f4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new columns with the results\n",
    "domains_mx_record_geolite2_df = domain_records_checked_df.select(domain_records_checked_df.top_level_domain,explode(domain_records_checked_df.mx_record_checked).alias('mx_record_checked'))\n",
    "domains_mx_record_geolite2_df = domains_mx_record_geolite2_df.withColumn(\"mx_record_ip\", udf_getARecords(\"mx_record_checked\")) \\\n",
    "                            .withColumn('mx_record_ip', explode(col('mx_record_ip'))) \\\n",
    "                            .withColumn(\"location\", udf_getGeoLite2_Location(\"mx_record_ip\")) \\\n",
    "                            .withColumn(\"asn\", udf_getGeoLite2_ASN(\"mx_record_ip\")) \\\n",
    "                            .select(\"top_level_domain\", \"mx_record_checked\", \"mx_record_ip\", \"location.*\", \"asn.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44719c-95bd-403e-a606-2d36052c70db",
   "metadata": {},
   "source": [
    "For storage in the PostgreSQL database, the first 15 rows of the data frame are displayed for checking for writing to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5722f-fcfb-4f22-a1a5-40b2b00c6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data frame\n",
    "domains_mx_record_geolite2_df.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d630a-0e0c-4f77-8064-d4810857e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data frame to the PostgreSQL database\n",
    "domains_mx_record_geolite2_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"domain_mx_record_geolite2\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e1bbb-a7b0-454a-9843-440119f71be0",
   "metadata": {},
   "source": [
    "## 4. IPv6 and SOA information fetch\n",
    "\n",
    "This section contains the fetch of IPv6, nameserver information and a ranking of the top ten master nameservers and the companies behind.\n",
    "The following block generates the required functions as user defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09db57-8d10-4626-b43f-0307bf12f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Funtions.ipynb\n",
    "from ipynb.fs.full.Functions import IPv6Record, IPv6Record_error, getSOAInformation, getSOAInformation_error, getNameServers, getNameServers_error\n",
    "\n",
    "udf_IPv6Record = udf(IPv6Record, BooleanType())\n",
    "udf_IPv6Record_error = udf(IPv6Record_error, IntegerType())\n",
    "udf_getSOAInformation = udf(getSOAInformation, ArrayType(StringType()))\n",
    "udf_getSOAInformation_error = udf(getSOAInformation_error, IntegerType())\n",
    "udf_getNameServers = udf(getNameServers, ArrayType(StringType()))\n",
    "udf_getNameServers_error = udf(getNameServers_error, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa67aa-8f10-4e7e-aff2-c3125f3f2ca8",
   "metadata": {},
   "source": [
    "### IPv6 information\n",
    "The IPv6 information provided by the following code are the availability of IPv6 (`ipv6_available`, boolean) and the possible error code during a request (`ipv6_error`). Therefore, the dataframe 'domains_ipv6_df' is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1947081-0ffd-421e-90c0-3ffdd8da90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df for ipv6 data\n",
    "domains_ipv6_df = domains_df.withColumn(\"ipv6_available\", udf_IPv6Record('top_level_domain'))\\\n",
    "                        .withColumn(\"ipv6_error\", udf_IPv6Record_error(\"top_level_domain\"))\\\n",
    "                        .drop('mx_record').drop('a_record')\n",
    "\n",
    "domains_ipv6_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756ea79-6c37-4bf1-8316-6bd18d5d9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data frame to the PostgreSQL database\n",
    "domains_ipv6_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"ip_v6_information\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781c555-87ea-4b75-8149-f151d7a34bf5",
   "metadata": {},
   "source": [
    "### SOA information\n",
    "Among the SOA record, the names of the nameservers `nameservers` (iincluding master server) are fetched in the following. The variable `nameservers_error` indicates if problems ocurred during the request wheres as `nameservers_count` contains the number of nameservers used per domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffc3d3-da61-4843-b822-d4e8216b3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df for SOA data and drop unnecessary columns\n",
    "domains_soa_df = domains_df.withColumn(\"soa_information\", udf_getSOAInformation(\"top_level_domain\"))\\\n",
    "                        .withColumn(\"soa_information_error\", udf_getSOAInformation_error(\"top_level_domain\"))\\\n",
    "                        .withColumn(\"nameservers\", udf_getNameServers(\"top_level_domain\"))\\\n",
    "                        .withColumn(\"nameservers_error\", udf_getNameServers_error(\"top_level_domain\"))\\\n",
    "                        .drop('mx_record').drop('a_record')\n",
    "\n",
    "# Add nameserver count variable\n",
    "def count_arr(arr): return 0 if arr == None else len(arr)\n",
    "count_arr_udf = udf(count_arr, IntegerType())\n",
    "domains_soa_df = domains_soa_df.withColumn(\"nameservers_count\", count_arr_udf('nameservers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a305bac-d297-49a4-93a1-f14e12c9def1",
   "metadata": {},
   "source": [
    "The details concerning the master nameserver are provided as ArrayType. As this is inconvenient with regard to the data structure (atomicity), the column `soa_infos_rep` is converted into a string type to separate its contents into separate variables.\n",
    "These are the name of the primary master nameserver `soa_name`, the refresh time of the SOA record executed by the secondary nameservers (`refresh`), the 'Time to live' (TTL), which defines how long a secondary nameserver caches the requested SOA information `minimum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c7617-e191-4b4a-bae8-76ff9c86dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ArrayType<String> into String as preparation for information separation\n",
    "domains_soa_df = domains_soa_df.withColumn(\"soa_infos_rep\", concat_ws(\" \", \"soa_information\"))\n",
    "\n",
    "# Split SOA information into separate columns (all String)\n",
    "split_col = split(domains_soa_df['soa_infos_rep'], ' ')\n",
    "domains_soa_df = domains_soa_df.withColumn('soa_name', split_col.getItem(0))\\\n",
    "                        .withColumn('refresh', split_col.getItem(3))\\\n",
    "                        .withColumn('minimum', split_col.getItem(6))\n",
    "\n",
    "# Helping function to catch empty entries concerning the master nameserver\n",
    "def replace_empty_strings(x):\n",
    "    return when(col(x) == \"\", None).otherwise(col(x))\n",
    "\n",
    "domains_soa_df = domains_soa_df.withColumn(\"soa_name\", replace_empty_strings(\"soa_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11b91e-03ec-4e3f-a0d3-e0d9835ed7f2",
   "metadata": {},
   "source": [
    "Some fetched entries contain unnecessary characters which need to be removed (dot at the end of the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaeff60-61cf-4a80-a192-2e0925e2dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last dot per soa mname\n",
    "domains_soa_df = domains_soa_df.withColumn('soa_name', regexp_replace('soa_name', '.$', ''))   \n",
    "\n",
    "# Remove last dot per nameserver entry\n",
    "lambda_dot_remove = lambda arr: [x[:-1] for x in arr]\n",
    "def fn_remove_dot(arr): return None if arr == None else lambda_dot_remove(arr)\n",
    "udf_remove_last_char_in_array = udf(fn_remove_dot, ArrayType(StringType()))\n",
    "\n",
    "domains_soa_df = domains_soa_df \\\n",
    "    .select(\"*\") \\\n",
    "    .withColumn('nameservers', udf_remove_last_char_in_array(col('nameservers')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221798f-1655-475a-8d7d-9c9ce30b122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type of the time setting columns into int\n",
    "domains_soa_df = domains_soa_df.withColumn(\"refresh\", domains_soa_df[\"refresh\"].cast(IntegerType()))\\\n",
    "                        .withColumn(\"minimum\", domains_soa_df[\"minimum\"].cast(IntegerType()))\\\n",
    "                        .drop('soa_information').drop('soa_infos_rep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ee32a-db7d-4e03-9316-989a5c93f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data frame to the PostgreSQL database\n",
    "domains_soa_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"soa\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857da4e9-aeac-4216-bc0d-a485a9846742",
   "metadata": {},
   "source": [
    "### Top ten master namservers and related company information\n",
    "This section creates a dataframe concerning the top ten master neameservers and the companies behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e47799-5e2f-4456-89a9-6175096359c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occrence of SOA records\n",
    "soa_name_count_top_ten_df = domains_soa_df.withColumn('soa_name', (col('soa_name'))) \\\n",
    "        .groupBy('soa_name') \\\n",
    "        .count()\n",
    "\n",
    "soa_name_count_top_ten_df = soa_name_count_top_ten_df.orderBy(['count'], ascending = [False]).limit(10)\n",
    "soa_name_count_top_ten_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998618a5-833e-49fe-ac2d-d5e95ce4b80b",
   "metadata": {},
   "source": [
    "The company informations per master nameserver will be added by a request of its IP adresses `ipv4`. The IP address(es) are the used within the functions concerning GeoLite2 to get the company information ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286d83b-e4a7-4b32-a040-22c205354263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove None entries (copmany information not available)\n",
    "soa_name_count_top_ten_df = soa_name_count_top_ten_df.na.drop(subset=[\"soa_name\"])\n",
    "\n",
    "soa_name_count_top_ten_df = soa_name_count_top_ten_df.withColumn(\"ipv4\", udf_getARecords(\"soa_name\"))\\\n",
    "                      .withColumn('ipv4', explode(col('ipv4'))) \\\n",
    "                      .withColumn(\"location\", udf_getGeoLite2_Location(\"ipv4\")) \\\n",
    "                      .withColumn(\"asn\", udf_getGeoLite2_ASN(\"ipv4\")) \\\n",
    "                      .select(\"soa_name\", \"count\", \"ipv4\", \"location.*\", \"asn.*\")\n",
    "#soa_name_count_top_ten_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c10d2-0bca-4b84-94e9-64929dedf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data frame to the PostgreSQL database\n",
    "soa_name_count_top_ten_df.repartition(8).write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection) \\\n",
    "    .option(\"dbtable\", \"soa_top_ten\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"batchsize\", 10000) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
